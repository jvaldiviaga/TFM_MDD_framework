#!/usr/bin/env python3
# ============================================================
# Script: analyze_batch_meg_raw.py
#
# OBJETIVO:
# Detectar posibles efectos batch (scanner, duraci√≥n, fecha de adquisici√≥n)
# en los archivos MEG crudos (.fif) ANTES del preprocesado.
# Esto permite anticipar problemas sistem√°ticos y adaptar decisiones t√©cnicas
# como el filtrado, correcci√≥n por ICA o exclusi√≥n.
#
# === FUNCIONES IMPLEMENTADAS ===
# ‚úÖ Extracci√≥n de metadatos t√©cnicos desde raw (duraci√≥n, fecha, n_projs)
# ‚úÖ Fusi√≥n con metadatos cl√≠nicos (scanner, grupo)
# ‚úÖ PCA con visualizaci√≥n por grupo y por scanner
# ‚úÖ Dendrograma jer√°rquico de similitud t√©cnica
# ‚úÖ C√°lculo de silhouette score para detecci√≥n objetiva de batch
# ‚úÖ Exportaci√≥n a batch_raw_summary.csv + PNGs
#
# === DISRUPTIVO RESPECTO A PIPELINES COMUNES ===
# ‚úÖ Eval√∫a el efecto batch antes del preprocesamiento
# ‚úÖ Ayuda a decidir si conviene armonizar o modificar la estrategia t√©cnica
# ‚úÖ Reduce gasto computacional en sujetos mal registrados
#
# === LIMITACIONES (CONSCIENTES) ===
# ‚ùå No eval√∫a diferencias funcionales (solo t√©cnicas)
# ‚ùå No armoniza autom√°ticamente (p.ej., neuroComBat)
# ‚úÖ Es un bloque exploratorio previo ‚Üí gu√≠a decisiones cl√≠nicas posteriores
# ============================================================

# Evaluamos el efecto batch tanto antes como despu√©s del preprocesado:
# - Antes: para detectar sesgos t√©cnicos estructurales (scanner, duraci√≥n, n_projs) que puedan comprometer el an√°lisis,
# - Despu√©s: para verificar si dichos sesgos persisten tras la limpieza (ICA, filtros) y decidir si aplicar correcci√≥n (e.g., neuroCombat).

# ======================== LIBRER√çAS ========================
import mne
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage, dendrogram
import argparse
import os
import seaborn as sns

# ===================== PAR√ÅMETROS DE ENTRADA =====================
parser = argparse.ArgumentParser(description="Evaluar efecto batch en MEG crudo.")
parser.add_argument('--data_dir', type=str, required=True,
                    help='Directorio con archivos *_meg.fif (crudos)')
parser.add_argument('--metadata_file', type=str, required=True,
                    help='Archivo Excel con columnas: URSI, Group, Scanner')
parser.add_argument('--output_dir', type=str, required=True,
                    help='Directorio de salida para resultados')
args = parser.parse_args()

data_dir = os.path.expanduser(args.data_dir)
output_dir = os.path.expanduser(args.output_dir)
os.makedirs(output_dir, exist_ok=True)

# ===================== CARGA Y FUSI√ìN DE METADATOS CL√çNICOS =====================
df_meta = pd.read_excel(args.metadata_file)
df_meta.columns = df_meta.columns.str.strip().str.lower()  # üîß Convertimos todas a min√∫scula

# Construir ID compatible con archivos .fif (usa 'ursi' en min√∫scula ahora)
df_meta['subject'] = 'M871' + df_meta['ursi'].astype(str).str.zfill(5)

# Renombrar campos si fuera necesario (opcional en este caso)
# df_meta = df_meta.rename(columns={'group': 'group', 'scanner': 'scanner'})

# Mostramos columnas detectadas (√∫til para debug)
print("üß© Columnas detectadas en df_meta:", df_meta.columns.tolist())

# Validaci√≥n de duplicados
if df_meta['subject'].duplicated().any():
    raise ValueError("‚ùå Hay IDs duplicados en df_meta['subject']. Revisa el Excel.")

# Verificamos columnas obligatorias
required_cols = ['subject', 'group', 'scanner']
missing = [col for col in required_cols if col not in df_meta.columns]
if missing:
    raise ValueError(f"‚ùå Faltan columnas clave en el Excel: {missing}")

# ===================== NOTA SOBRE GENERALIZACI√ìN =====================
# Este bloque est√° adaptado al dataset AIM/Reward.
# Si se utiliza otro Excel, es necesario revisar:
# - Los nombres de columnas
# - El encoding invisible (espacios, tildes, may√∫sculas)
# - Que 'group' y 'scanner' no contengan NaN ni codificaciones no est√°ndar


# ===================== EXTRACCI√ìN DE INFORMACI√ìN T√âCNICA =====================
info_list = []
for fname in sorted(os.listdir(data_dir)):
    if fname.startswith("sub-") and fname.endswith("_meg.fif"):
        print(f"üìÇ Leyendo archivo: {fname}", flush=True)  # Muestra nombre antes de intentar abrir

        path = os.path.join(data_dir, fname)
        subj = os.path.basename(fname).split("_")[0].replace("sub-", "").strip()

        try:
            print(f"üîÑ Extrayendo info t√©cnica de {subj}...", flush=True)
            raw = mne.io.read_raw_fif(path, preload=False, verbose="ERROR")

            meas_date = raw.info['meas_date']
            duration = raw.n_times / raw.info['sfreq']
            sfreq = raw.info['sfreq']
            n_projs = len(raw.info['projs'])
            proj_types = ','.join(set(p['desc'] for p in raw.info['projs'])) if raw.info['projs'] else 'None'

            info_list.append({
                "subject": subj,
                "meas_date": str(meas_date.date()) if meas_date else "unknown",
                "duration_sec": round(duration, 2),
                "sfreq": sfreq,
                "n_projs": n_projs,
                "proj_types": proj_types
            })

            print(f"‚úÖ {subj} procesado", flush=True)

        except Exception as e:
            print(f"‚ùå Error leyendo {fname}: {e}", flush=True)


df_info = pd.DataFrame(info_list)
print("‚úÖ IDs en df_info (de archivos):", df_info['subject'].unique()[:5])
print("‚úÖ IDs en df_meta (desde Excel):", df_meta['subject'].unique()[:5])

# ===================== FUSI√ìN Y LIMPIEZA =====================
# Homogeneizamos los IDs a may√∫sculas sin espacios
df_info['subject'] = df_info['subject'].astype(str).str.upper().str.strip()
df_meta['subject'] = df_meta['subject'].astype(str).str.upper().str.strip()

# Hacemos el merge cl√≠nico-t√©cnico por ID homog√©neo
df = df_info.merge(df_meta, how='left', on='subject')

# Normalizamos columnas
df['group'] = df['group'].astype(str).str.strip()
df['scanner'] = pd.Categorical(df['scanner'])  # Categor√≠a necesaria para silhouette_score

# Eliminamos sujetos sin metadatos cl√≠nicos v√°lidos
df = df.dropna(subset=['group', 'scanner'])

if df.empty:
    raise ValueError("‚ùå No hay sujetos v√°lidos tras el merge. Revisa IDs o columnas del Excel.")

# Diagn√≥stico en consola
print(f"üë• Sujetos crudos detectados: {len(df_info)}")
print(f"üìã Sujetos con metadatos v√°lidos: {len(df)}")
print("üìä Grupos detectados:", df['group'].unique())
print("üß™ Scanners detectados:", df['scanner'].unique())

# ===================== PCA =====================
features = ['duration_sec', 'sfreq', 'n_projs']
X = StandardScaler().fit_transform(df[features])
pca = PCA(n_components=2)
df[['PC1', 'PC2']] = pca.fit_transform(X)

# PCA combinado
sns.set(style="whitegrid", font_scale=1.1)
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="PC1", y="PC2", hue="group", style="scanner", palette="Set2", s=100)
plt.title("PCA de sujetos MEG crudos: duraci√≥n / scanner / proyecciones")
plt.xlabel(f"PC1 ({round(pca.explained_variance_ratio_[0]*100, 1)}%)")
plt.ylabel(f"PC2 ({round(pca.explained_variance_ratio_[1]*100, 1)}%)")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "pca_batch_effect_raw.png"), dpi=150)
plt.close()

# PCA por grupo
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="PC1", y="PC2", hue="group", palette="Set2", s=100)
plt.title("PCA coloreado por grupo cl√≠nico")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "pca_by_group.png"), dpi=150)
plt.close()

# PCA por scanner
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="PC1", y="PC2", hue=df['scanner'].astype(str), palette="Set1", s=100)
plt.title("PCA coloreado por scanner (batch)")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "pca_by_scanner.png"), dpi=150)
plt.close()

# ===================== DENDROGRAMA =====================
Z = linkage(X, method='ward')
plt.figure(figsize=(12, 6))
dendrogram(Z, labels=df['subject'].tolist(), leaf_rotation=90, leaf_font_size=9)
plt.title("Dendrograma jer√°rquico: agrupaci√≥n t√©cnica")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "dendrogram_batch_raw.png"), dpi=150)
plt.close()

# ===================== DETECCI√ìN AUTOM√ÅTICA DE BATCH =====================
try:
    sil_score = silhouette_score(X, df['scanner'].cat.codes)
    print(f"üîç Silhouette score por scanner (batch): {sil_score:.2f}")
    if sil_score > 0.3:
        print("‚ö†Ô∏è Posible efecto batch detectado.")
    else:
        print("‚úÖ No se detecta efecto batch claro.")
except Exception as e:
    print(f"‚ö†Ô∏è No se pudo calcular silhouette score: {e}")

# ===================== EXPORTACI√ìN =====================
df.to_csv(os.path.join(output_dir, "batch_raw_summary.csv"), index=False)
print("‚úÖ Resultados guardados en:", output_dir)
